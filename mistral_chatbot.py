from llama_cpp import Llama

# âœ… Load your local model (Q2_K version)
llm = Llama(
    model_path="mistral-7b-instruct-v0.1.Q2_K.gguf",
    n_ctx=2048,
    n_threads=4  # Adjust if you have more CPU cores
)

print("ğŸ¤– Mistral GPT Chatbot Ready!")
print("Type 'exit' to quit.\n")

while True:
    prompt = input("ğŸ§‘ You: ")

    if prompt.lower() == "exit":
        print("ğŸ‘‹ Goodbye!")
        break

    print("ğŸ§  Generating response...\n")

    try:
        # âœ… Instruction format required by Mistral
        full_prompt = f"[INST] {prompt.strip()} [/INST]"

        output = llm(full_prompt, max_tokens=300)
        reply = output['choices'][0]['text']
        print(f"ğŸ¤– Mistral: {reply.strip()}\n")

    except Exception as e:
        print(f"âš ï¸ Error: {e}")
